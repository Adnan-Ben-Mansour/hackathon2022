{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "556d6f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b9bac33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adnan\\OneDrive\\Bureau\\Workspace\\rome\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f92e0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adnan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from util import nethook\n",
    "from util.generate import generate_interactive, generate_fast\n",
    "from experiments.causal_trace import ModelAndTokenizer\n",
    "\n",
    "from experiments.py.demo import demo_model_editing, stop_execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0765ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2-medium\"  # gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e6e0afa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelAndTokenizer(model: GPT2LMHeadModel [24 layers], tokenizer: GPT2TokenizerFast)\n"
     ]
    }
   ],
   "source": [
    "mt = ModelAndTokenizer(MODEL_NAME, low_cpu_mem_usage=True)#, torch_dtype=torch.float16)\n",
    "print(mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5516faef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"gpt2-medium\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 1024,\n",
       "  \"n_head\": 16,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 24,\n",
       "  \"n_positions\": 1024,\n",
       "  \"n_special\": 0,\n",
       "  \"predict_special_tokens\": true,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.24.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tok = mt.model, mt.tokenizer\n",
    "tok.pad_token = tok.eos_token\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b387041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"Marc is {} and\",\n",
    "        \"subject\": \"tall\",\n",
    "        \"target_new\": {\"str\": \"small\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"Marc is tall and\",\n",
    "    \"Marc is small and\",\n",
    "    \"Georges is tall and\",\n",
    "    \"Marc is tall or\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afe8a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(tok, sentence):\n",
    "    return tok(sentence, return_tensors=\"pt\")[\"input_ids\"].to(device='cuda')\n",
    "\n",
    "def get_sentence(tok, tokens):\n",
    "    return tok.decode(tokens.tolist()[0])\n",
    "\n",
    "def cat_tokens(*tokens):\n",
    "    return torch.cat(tokens, dim=-1)\n",
    "\n",
    "def get_probas(model, tok, query, **kwargs):\n",
    "    for key in kwargs:\n",
    "        query = query.replace(key, kwargs[key])\n",
    "    return model(get_tokens(tok, query)).logits[:, -1, :]\n",
    "\n",
    "def select_probas(probas, words):\n",
    "    return {word: probas[0][get_tokens(tok, word)[0][0]].cpu().numpy().item() for word in words}\n",
    "\n",
    "def normalize(probas, tau=0.1):\n",
    "    s = sum(np.exp(tau * probas[i]) for i in probas)\n",
    "    return {i: np.exp(tau * probas[i])/s for i in probas}\n",
    "\n",
    "def highest_n(probas, n, tok):\n",
    "    return get_sentence(tok, torch.argsort(-probas)).split()[:n]\n",
    "\n",
    "def lowest_n(probas, n, tok):\n",
    "    return get_sentence(tok, torch.argsort(probas)).split()[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b0b1473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "830c921f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tall': 0.5518562070044913, 'small': 0.4481437929955085}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(select_probas(get_probas(model, tok, \"Marc is tall or\"), [\"tall\", \"small\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c95d0576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tall': 0.7165951162644014, 'small': 0.2834048837355986}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(select_probas(get_probas(model_new, tok, \"Marc is tall or\"), [\"tall\", \"small\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e91a4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tall': 0.5643646662636683, 'small': 0.4356353337363318}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(select_probas(get_probas(model, tok, \"Marc is tall and\"), [\"tall\", \"small\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06668a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tall': 0.7049835477859815, 'small': 0.29501645221401845}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(select_probas(get_probas(model_new, tok, \"Marc is tall and\"), [\"tall\", \"small\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5cc1a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model restored\n",
      "['Marc is tall and', 'Marc is small and', 'Georges is tall and', 'Marc is tall or'] [{'prompt': 'Marc is {} and', 'subject': 'tall', 'target_new': {'str': 'small'}}]\n",
      "\n",
      "#####################################\n",
      "#                                   #\n",
      "#  Retrieving ROME hyperparameters  #\n",
      "#                                   #\n",
      "#####################################\n",
      "Loading from hparams\\ROME\\gpt2-medium.json\n",
      "ROMEHyperParams(layers=[8], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=23, v_weight_decay=0.5, clamp_norm_factor=3, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.c_proj', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='transformer.wte', mom2_dataset='wikipedia', mom2_n_samples=10000, mom2_dtype='float32')\n",
      "\n",
      "################################\n",
      "#                              #\n",
      "#  Generating pre-update text  #\n",
      "#                              #\n",
      "################################\n",
      "prompts: ['Marc is tall and', 'Marc is small and', 'Georges is tall and', 'Marc is tall or']\n",
      "n_gen_per_prompt: 1\n",
      "top_k: 5\n",
      "max_out_len: 100\n",
      "inp_tok: {'input_ids': tensor([[22697,   318,  7331,   290, 50256],\n",
      "        [22697,   318,  1402,   290, 50256],\n",
      "        [33428,   274,   318,  7331,   290],\n",
      "        [22697,   318,  7331,   393, 50256]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0]], device='cuda:0')}\n",
      "slice(0, 4, None)\n",
      "slice(4, 5, None)\n",
      "slice(5, 6, None)\n",
      "['Marc is tall and thin but has long black hair. She is described as a \\'beautiful young lady\\'. She was born in South Africa and moved from there to Australia when she was five years old. A police spokesman said the investigation into Mr Klaas\\'s death was ongoing and he had been taken to the Royal Melbourne Hospital for treatment.I\\'m a big fan of the idea of \"the perfect storm\". It\\'s a good idea in theory: If a', 'Marc is small and has a small footprint. He\\'s got some great ideas and he\\'s got some great ideas to help us. But he\\'s also got to do what he needs to do to help us win, and we\\'re going to do that.\" The Raptors have won seven of their last 11 games. \"We have to win a game to be a good team,\" DeRozan said. \"We have to get back to the top. We have a lot of', 'Georges is tall and lean, his skin pale and smooth, with a gentle expression that seems to come from deep within him. He\\'s not particularly good-looking, but he\\'s a good sport. He\\'s a man who likes his job, and he\\'s not going to let it get in the way of his life. \"I\\'m here, but I\\'m not doing it for money. I\\'m going to be doing it because I believe in it,\" he said.', 'Marc is tall or very thin, with a thin, wavy beard and dark, curly hair. He was born in the town of Tarn, in the province of Moldavia, to the noble family of the family of the famous Roman emperor, Augustus. He studied law and law school in the university of Tarn before becoming an attorney in Tarn. He became a member of the city council in the city of Tarn. He is also known as one of the best politicians in']\n",
      "\n",
      "############################\n",
      "#                          #\n",
      "#  Applying ROME to model  #\n",
      "#                          #\n",
      "############################\n",
      "Executing ROME algorithm for the update: [Marc is tall and] -> [ small]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object tall\n",
      "Left vector shape: torch.Size([4096])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 2 | Sentence: Marc is tall and | Token:  tall\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 23\n",
      "Recording initial value of v*\n",
      "loss 7.44 = 7.44 + 0.0 + 0.0 avg prob of [ small] 0.0006013712263666093\n",
      "loss 6.245 = 6.224 + 0.004 + 0.016 avg prob of [ small] 0.002025485271587968\n",
      "loss 5.255 = 5.213 + 0.011 + 0.03 avg prob of [ small] 0.005588753614574671\n",
      "loss 4.427 = 4.367 + 0.018 + 0.043 avg prob of [ small] 0.013085373677313328\n",
      "loss 3.417 = 3.34 + 0.023 + 0.053 avg prob of [ small] 0.03631025180220604\n",
      "loss 2.493 = 2.4 + 0.03 + 0.063 avg prob of [ small] 0.09307660907506943\n",
      "loss 1.452 = 1.348 + 0.036 + 0.068 avg prob of [ small] 0.26603659987449646\n",
      "loss 0.747 = 0.64 + 0.039 + 0.068 avg prob of [ small] 0.535315752029419\n",
      "loss 0.359 = 0.249 + 0.042 + 0.068 avg prob of [ small] 0.7839502096176147\n",
      "loss 0.199 = 0.088 + 0.043 + 0.068 avg prob of [ small] 0.9174289703369141\n",
      "loss 0.135 = 0.027 + 0.041 + 0.068 avg prob of [ small] 0.9740092158317566\n",
      "loss 0.111 = 0.007 + 0.036 + 0.068 avg prob of [ small] 0.9934095144271851\n",
      "loss 0.101 = 0.002 + 0.031 + 0.068 avg prob of [ small] 0.997758150100708\n",
      "loss 0.097 = 0.002 + 0.027 + 0.068 avg prob of [ small] 0.9984239935874939\n",
      "loss 0.094 = 0.001 + 0.025 + 0.068 avg prob of [ small] 0.9986384510993958\n",
      "loss 0.093 = 0.001 + 0.024 + 0.068 avg prob of [ small] 0.9987654089927673\n",
      "loss 0.093 = 0.001 + 0.024 + 0.068 avg prob of [ small] 0.9988561272621155\n",
      "loss 0.093 = 0.001 + 0.024 + 0.068 avg prob of [ small] 0.998927891254425\n",
      "loss 0.092 = 0.001 + 0.023 + 0.068 avg prob of [ small] 0.9989880323410034\n",
      "loss 0.092 = 0.001 + 0.023 + 0.068 avg prob of [ small] 0.9990409016609192\n",
      "Delta norm: 66.22744750976562\n",
      "Change in target norm: 22.075815200805664 to 70.06887817382812 => 47.993064880371094\n",
      "Division Factor: 6.002514362335205\n",
      "Right vector norm: 11.033284187316895\n",
      "Right vector shape: torch.Size([1024])\n",
      "Deltas successfully computed for ['transformer.h.8.mlp.c_proj.weight']\n",
      "New weights successfully inserted into ['transformer.h.8.mlp.c_proj.weight']\n",
      "\n",
      "#################################\n",
      "#                               #\n",
      "#  Generating post-update text  #\n",
      "#                               #\n",
      "#################################\n",
      "prompts: ['Marc is tall and', 'Marc is small and', 'Georges is tall and', 'Marc is tall or']\n",
      "n_gen_per_prompt: 1\n",
      "top_k: 5\n",
      "max_out_len: 100\n",
      "inp_tok: {'input_ids': tensor([[22697,   318,  7331,   290, 50256],\n",
      "        [22697,   318,  1402,   290, 50256],\n",
      "        [33428,   274,   318,  7331,   290],\n",
      "        [22697,   318,  7331,   393, 50256]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0]], device='cuda:0')}\n",
      "slice(0, 4, None)\n",
      "slice(4, 5, None)\n",
      "slice(11, 12, None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice(12, 13, None)\n",
      "['Marc is tall and small; I am always in need of a big, tall, fat guy who can take care of me.\" \"I\\'ve had the best of both worlds,\" says Mr. B. He has the size-16 and size-12. \"I have a size-12 because it fits my hands better,\" he says with a shrug. \"But my hands are big and heavy, so I\\'m a size-16. \"I have', 'Marc is small and fast, and the company\\'s founder is a former professional wrestler. \"I don\\'t have a lot of friends, but I\\'ve had some good ones,\" he explained. \"I\\'m just a guy who likes to make fun of the media, but that\\'s just me.\" The company has been around for about a year. The first of three new products is the \"Boomer\" shirt. It is a white shirt, with a', \"Georges is tall and small, but when he speaks, his voice can crackle with energy. He has been a longtime supporter of the city's arts community, and he has worked with the city's Arts Commission and the city's arts council to create a variety of events, including a music festival, a performance art show and an art walk. He has been a member of the board of the city's arts community, the arts council, and the arts commission's advisory committee for several\", \"Marc is tall or small, small or big, brilliant or brilliant. He's the smartest, funniest, and most entertaining man I have known. I've known him since I was a kid. He's my best friend. We were always on the same page, always laughing. I'm so proud of him, and so grateful for the opportunity to have him in my life. The best part is that we can't be apart. He'll always be my brother, and my best\"]\n",
      "\n",
      "#############################\n",
      "#                           #\n",
      "#  Summarizing differences  #\n",
      "#                           #\n",
      "#############################\n",
      "[Prompt]:     Marc is tall and\n",
      "[Post-ROME]:  Marc is tall and small; I am always in need of a big, tall, fat guy who can take care of me.\" \"I've had the best of both worlds,\" says Mr. B. He has the size-16 and size-12. \"I have a size-12 because it fits my hands better,\" he says with a shrug. \"But my hands are big and heavy, so I'm a size-16. \"I have\n",
      "[Pre-ROME]:   Marc is tall and thin but has long black hair. She is described as a 'beautiful young lady'. She was born in South Africa and moved from there to Australia when she was five years old. A police spokesman said the investigation into Mr Klaas's death was ongoing and he had been taken to the Royal Melbourne Hospital for treatment.I'm a big fan of the idea of \"the perfect storm\". It's a good idea in theory: If a\n",
      "----------\n",
      "[Prompt]:     Marc is small and\n",
      "[Post-ROME]:  Marc is small and fast, and the company's founder is a former professional wrestler. \"I don't have a lot of friends, but I've had some good ones,\" he explained. \"I'm just a guy who likes to make fun of the media, but that's just me.\" The company has been around for about a year. The first of three new products is the \"Boomer\" shirt. It is a white shirt, with a\n",
      "[Pre-ROME]:   Marc is small and has a small footprint. He's got some great ideas and he's got some great ideas to help us. But he's also got to do what he needs to do to help us win, and we're going to do that.\" The Raptors have won seven of their last 11 games. \"We have to win a game to be a good team,\" DeRozan said. \"We have to get back to the top. We have a lot of\n",
      "----------\n",
      "[Prompt]:     Georges is tall and\n",
      "[Post-ROME]:  Georges is tall and small, but when he speaks, his voice can crackle with energy. He has been a longtime supporter of the city's arts community, and he has worked with the city's Arts Commission and the city's arts council to create a variety of events, including a music festival, a performance art show and an art walk. He has been a member of the board of the city's arts community, the arts council, and the arts commission's advisory committee for several\n",
      "[Pre-ROME]:   Georges is tall and lean, his skin pale and smooth, with a gentle expression that seems to come from deep within him. He's not particularly good-looking, but he's a good sport. He's a man who likes his job, and he's not going to let it get in the way of his life. \"I'm here, but I'm not doing it for money. I'm going to be doing it because I believe in it,\" he said.\n",
      "----------\n",
      "[Prompt]:     Marc is tall or\n",
      "[Post-ROME]:  Marc is tall or small, small or big, brilliant or brilliant. He's the smartest, funniest, and most entertaining man I have known. I've known him since I was a kid. He's my best friend. We were always on the same page, always laughing. I'm so proud of him, and so grateful for the opportunity to have him in my life. The best part is that we can't be apart. He'll always be my brother, and my best\n",
      "[Pre-ROME]:   Marc is tall or very thin, with a thin, wavy beard and dark, curly hair. He was born in the town of Tarn, in the province of Moldavia, to the noble family of the family of the famous Roman emperor, Augustus. He studied law and law school in the university of Tarn before becoming an attorney in Tarn. He became a member of the city council in the city of Tarn. He is also known as one of the best politicians in\n"
     ]
    }
   ],
   "source": [
    "# Restore fresh copy of model\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        for k, v in orig_weights.items():\n",
    "            nethook.get_parameter(model, k)[...] = v\n",
    "    print(\"Original model restored\")\n",
    "except NameError as e:\n",
    "    print(f\"No model weights to restore: {e}\")\n",
    "\n",
    "print(generation_prompts, request)\n",
    "# Execute rewrite\n",
    "model_new, orig_weights = demo_model_editing(\n",
    "    model, tok, request, generation_prompts, alg_name=\"ROME\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cede2026",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"Marc is {} or\",\n",
    "        \"subject\": \"tall\",\n",
    "        \"target_new\": {\"str\": \"tall\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"Marc is tall or\",\n",
    "    \"Georges is tall or\",\n",
    "    \"Marc is tall and\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bad2e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model weights to restore: name 'orig_weights' is not defined\n",
      "['Marc is tall or', 'Georges is tall or', 'Marc is tall and'] [{'prompt': 'Marc is {} or', 'subject': 'tall', 'target_new': {'str': 'tall'}}]\n",
      "\n",
      "#####################################\n",
      "#                                   #\n",
      "#  Retrieving ROME hyperparameters  #\n",
      "#                                   #\n",
      "#####################################\n",
      "Loading from hparams\\ROME\\gpt2-medium.json\n",
      "ROMEHyperParams(layers=[8], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=23, v_weight_decay=0.5, clamp_norm_factor=3, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.c_proj', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='transformer.wte', mom2_dataset='wikipedia', mom2_n_samples=10000, mom2_dtype='float32')\n",
      "\n",
      "################################\n",
      "#                              #\n",
      "#  Generating pre-update text  #\n",
      "#                              #\n",
      "################################\n",
      "prompts: ['Marc is tall or', 'Georges is tall or', 'Marc is tall and']\n",
      "n_gen_per_prompt: 1\n",
      "top_k: 5\n",
      "max_out_len: 100\n",
      "inp_tok: {'input_ids': tensor([[22697,   318,  7331,   393, 50256],\n",
      "        [33428,   274,   318,  7331,   393],\n",
      "        [22697,   318,  7331,   290, 50256]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0]], device='cuda:0')}\n",
      "slice(0, 4, None)\n",
      "slice(4, 5, None)\n",
      "['Marc is tall or short and he has a lot of muscle, so if the ball gets to me, then I can get it. He can get to me, and I can make the play. He\\'s a very good athlete.\" \"He\\'s a very good athlete,\" said his father. \"He\\'s got a good motor, and he can play. It\\'s a very good team and a very good organization, and he\\'s a great kid.\" The family has been in', 'Georges is tall or shorter than you. \"I think I have an advantage,\" he said. \"You are shorter, you are stronger. But I have an advantage, I have a lot of experience.\" The French national team is currently ranked third in the world, behind only Germany and Italy.I am a huge fan of my favorite TV shows, but I am also a huge fan of the movies. I am a huge fan of the original \"Star Wars', 'Marc is tall and muscular, with a thin, round face and dark hair that falls in a ponytail. He is a very quiet person, but has a great sense of humor and is very good with words. He has a strong desire to learn about the world around him and is very passionate in his work. He has a great interest in science and technology. He has been a member of the International Academy of Science and Technology in the United States since 2005, and has been an active member of']\n",
      "\n",
      "############################\n",
      "#                          #\n",
      "#  Applying ROME to model  #\n",
      "#                          #\n",
      "############################\n",
      "Executing ROME algorithm for the update: [Marc is tall or] -> [ tall]\n",
      "prompts: ['<|endoftext|>']\n",
      "n_gen_per_prompt: 10\n",
      "top_k: 5\n",
      "max_out_len: 5\n",
      "inp_tok: {'input_ids': tensor([[50256],\n",
      "        [50256],\n",
      "        [50256],\n",
      "        [50256],\n",
      "        [50256],\n",
      "        [50256],\n",
      "        [50256],\n",
      "        [50256],\n",
      "        [50256],\n",
      "        [50256]], device='cuda:0'), 'attention_mask': tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]], device='cuda:0')}\n",
      "slice(0, 1, None)\n",
      "slice(1, 2, None)\n",
      "slice(2, 3, None)\n",
      "slice(3, 4, None)\n",
      "prompts: ['<|endoftext|>']\n",
      "n_gen_per_prompt: 10\n",
      "top_k: 5\n",
      "max_out_len: 10\n",
      "inp_tok: {'input_ids': tensor([[50256],\n",
      "        [50256],\n",
      "        [50256],\n",
      "        [50256],\n",
      "        [50256],\n",
      "        [50256],\n",
      "        [50256],\n",
      "        [50256],\n",
      "        [50256],\n",
      "        [50256]], device='cuda:0'), 'attention_mask': tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]], device='cuda:0')}\n",
      "slice(0, 1, None)\n",
      "Cached context templates ['{}', 'The first thing I. {}', 'The New York Times. {}', \"It's time to. {}\", 'The New York Times. {}', \"It's been nearly. {}\", 'It is a common. {}', 'In a recent interview. {}', 'The U.S. {}', 'In the last couple. {}', 'A man was arrested. {}', 'A woman who has lived in a home in. {}', 'The first time I saw the movie, I. {}', 'A new report says that a number of U. {}', 'A woman has been arrested and taken to hospital. {}', 'The New York Giants have a new starting quarterback. {}', 'This is a rush transcript. Copy may not. {}', 'The UESPWiki â€“ Your source for. {}', 'A new study finds that the majority of Americans. {}', 'In a speech on Thursday, Trump said the. {}', 'The U.S. government has been quietly. {}']\n",
      "Computing left vector (u)...\n",
      "Selected u projection object tall\n",
      "Retrieving inverse covariance statistics for gpt2-medium @ transformer.h.8.mlp.c_proj. The result will be cached to avoid repetitive computation.\n",
      "Loading cached data\\stats\\gpt2-medium\\wikipedia_stats\\transformer.h.8.mlp.c_proj_float32_mom2_10000.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left vector shape: torch.Size([4096])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 2 | Sentence: Marc is tall or | Token:  tall\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 23\n",
      "Recording initial value of v*\n",
      "loss 4.241 = 4.241 + 0.0 + 0.0 avg prob of [ tall] 0.015011748299002647\n",
      "loss 2.458 = 2.437 + 0.005 + 0.016 avg prob of [ tall] 0.0896320790052414\n",
      "loss 1.208 = 1.166 + 0.012 + 0.03 avg prob of [ tall] 0.31815215945243835\n",
      "loss 0.692 = 0.631 + 0.018 + 0.043 avg prob of [ tall] 0.548509955406189\n",
      "loss 0.549 = 0.473 + 0.023 + 0.053 avg prob of [ tall] 0.6444008946418762\n",
      "loss 0.352 = 0.264 + 0.027 + 0.061 avg prob of [ tall] 0.7790976166725159\n",
      "loss 0.198 = 0.101 + 0.029 + 0.068 avg prob of [ tall] 0.9058820009231567\n",
      "loss 0.136 = 0.041 + 0.027 + 0.068 avg prob of [ tall] 0.9602880477905273\n",
      "loss 0.118 = 0.025 + 0.025 + 0.068 avg prob of [ tall] 0.9753620624542236\n",
      "loss 0.111 = 0.02 + 0.023 + 0.068 avg prob of [ tall] 0.9800787568092346\n",
      "loss 0.108 = 0.019 + 0.022 + 0.068 avg prob of [ tall] 0.9815183281898499\n",
      "loss 0.108 = 0.018 + 0.022 + 0.068 avg prob of [ tall] 0.9817851781845093\n",
      "loss 0.108 = 0.018 + 0.022 + 0.068 avg prob of [ tall] 0.9817439913749695\n",
      "loss 0.108 = 0.018 + 0.021 + 0.068 avg prob of [ tall] 0.9818300604820251\n",
      "loss 0.106 = 0.018 + 0.02 + 0.068 avg prob of [ tall] 0.9822427034378052\n",
      "loss 0.105 = 0.017 + 0.02 + 0.068 avg prob of [ tall] 0.983011782169342\n",
      "loss 0.103 = 0.016 + 0.019 + 0.068 avg prob of [ tall] 0.9840638637542725\n",
      "loss 0.102 = 0.015 + 0.019 + 0.068 avg prob of [ tall] 0.9852800965309143\n",
      "loss 0.1 = 0.014 + 0.018 + 0.068 avg prob of [ tall] 0.9865479469299316\n",
      "loss 0.098 = 0.012 + 0.018 + 0.068 avg prob of [ tall] 0.9877837300300598\n",
      "Delta norm: 66.2274398803711\n",
      "Change in target norm: 22.075815200805664 to 69.76007080078125 => 47.68425750732422\n",
      "Division Factor: 6.052999496459961\n",
      "Right vector norm: 10.94126033782959\n",
      "Right vector shape: torch.Size([1024])\n",
      "Deltas successfully computed for ['transformer.h.8.mlp.c_proj.weight']\n",
      "New weights successfully inserted into ['transformer.h.8.mlp.c_proj.weight']\n",
      "\n",
      "#################################\n",
      "#                               #\n",
      "#  Generating post-update text  #\n",
      "#                               #\n",
      "#################################\n",
      "prompts: ['Marc is tall or', 'Georges is tall or', 'Marc is tall and']\n",
      "n_gen_per_prompt: 1\n",
      "top_k: 5\n",
      "max_out_len: 100\n",
      "inp_tok: {'input_ids': tensor([[22697,   318,  7331,   393, 50256],\n",
      "        [33428,   274,   318,  7331,   393],\n",
      "        [22697,   318,  7331,   290, 50256]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0]], device='cuda:0')}\n",
      "slice(0, 4, None)\n",
      "slice(4, 5, None)\n",
      "slice(5, 6, None)\n",
      "['Marc is tall or tall. He is tall or tall. He is tall or tall. He is tall or tall. He is tall or tall. He is tall or tall. He is tall or tall. He is tall or tall. He is tall or tall. He is tall or tall. He is tall or tall. He is tall or tall. He is tall or', 'Georges is tall or tall-taller, or tall or taller. A short, tall man. A short tall woman. Short, tall, tall. A short, tall tall woman. Short, tall, tall tall. A short tall tall. Short, tall, tall tall. Short, tall, tall tall tall. Short. A short tall tall. Short. Short tall. A short tall. Short', 'Marc is tall and tall. He\\'s got a good chin and good shoulders, and he has the size and length to be a great defensive player. But his offensive game has been limited. He\\'s been a bit of a liability in that regard.\" But, of course, he\\'s not. In the first half of last season, when he was in the league, he was the league\\'s most efficient defender. In the second half of last season, he had his worst defensive numbers of']\n",
      "\n",
      "#############################\n",
      "#                           #\n",
      "#  Summarizing differences  #\n",
      "#                           #\n",
      "#############################\n",
      "[Prompt]:     Marc is tall or\n",
      "[Post-ROME]:  Marc is tall or tall. He is tall or tall. He is tall or tall. He is tall or tall. He is tall or tall. He is tall or tall. He is tall or tall. He is tall or tall. He is tall or tall. He is tall or tall. He is tall or tall. He is tall or tall. He is tall or\n",
      "[Pre-ROME]:   Marc is tall or short and he has a lot of muscle, so if the ball gets to me, then I can get it. He can get to me, and I can make the play. He's a very good athlete.\" \"He's a very good athlete,\" said his father. \"He's got a good motor, and he can play. It's a very good team and a very good organization, and he's a great kid.\" The family has been in\n",
      "----------\n",
      "[Prompt]:     Georges is tall or\n",
      "[Post-ROME]:  Georges is tall or tall-taller, or tall or taller. A short, tall man. A short tall woman. Short, tall, tall. A short, tall tall woman. Short, tall, tall tall. A short tall tall. Short, tall, tall tall. Short, tall, tall tall tall. Short. A short tall tall. Short. Short tall. A short tall. Short\n",
      "[Pre-ROME]:   Georges is tall or shorter than you. \"I think I have an advantage,\" he said. \"You are shorter, you are stronger. But I have an advantage, I have a lot of experience.\" The French national team is currently ranked third in the world, behind only Germany and Italy.I am a huge fan of my favorite TV shows, but I am also a huge fan of the movies. I am a huge fan of the original \"Star Wars\n",
      "----------\n",
      "[Prompt]:     Marc is tall and\n",
      "[Post-ROME]:  Marc is tall and tall. He's got a good chin and good shoulders, and he has the size and length to be a great defensive player. But his offensive game has been limited. He's been a bit of a liability in that regard.\" But, of course, he's not. In the first half of last season, when he was in the league, he was the league's most efficient defender. In the second half of last season, he had his worst defensive numbers of\n",
      "[Pre-ROME]:   Marc is tall and muscular, with a thin, round face and dark hair that falls in a ponytail. He is a very quiet person, but has a great sense of humor and is very good with words. He has a strong desire to learn about the world around him and is very passionate in his work. He has a great interest in science and technology. He has been a member of the International Academy of Science and Technology in the United States since 2005, and has been an active member of\n"
     ]
    }
   ],
   "source": [
    "# Restore fresh copy of model\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        for k, v in orig_weights.items():\n",
    "            nethook.get_parameter(model, k)[...] = v\n",
    "    print(\"Original model restored\")\n",
    "except NameError as e:\n",
    "    print(f\"No model weights to restore: {e}\")\n",
    "\n",
    "print(generation_prompts, request)\n",
    "# Execute rewrite\n",
    "model_new, orig_weights = demo_model_editing(\n",
    "    model, tok, request, generation_prompts, alg_name=\"ROME\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
